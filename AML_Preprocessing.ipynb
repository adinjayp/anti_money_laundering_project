{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4656972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatable as dt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datatable import f, join, sort\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def initial_preprocessing(raw_data, first_timestamp):\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    currency_dict = {}\n",
    "    payment_format_dict = {}\n",
    "    bank_account_dict = {}\n",
    "    account_dict = {}\n",
    "\n",
    "    def get_dict_value(name, collection):\n",
    "        if name in collection:\n",
    "            value = collection[name]\n",
    "        else:\n",
    "            value = len(collection)\n",
    "            collection[name] = value\n",
    "        return value\n",
    "\n",
    "    for i in range(raw_data.nrows):\n",
    "        datetime_object = datetime.strptime(raw_data[i, \"Timestamp\"], '%Y/%m/%d %H:%M')\n",
    "        timestamp = datetime_object.timestamp()\n",
    "        day = datetime_object.day\n",
    "        month = datetime_object.month\n",
    "        year = datetime_object.year\n",
    "\n",
    "        if first_timestamp == -1:\n",
    "            start_time = datetime(year, month, day)\n",
    "            first_timestamp = start_time.timestamp() - 10\n",
    "\n",
    "        timestamp = timestamp - first_timestamp\n",
    "\n",
    "        receiving_currency = get_dict_value(raw_data[i, \"Receiving Currency\"], currency_dict)\n",
    "        payment_currency = get_dict_value(raw_data[i, \"Payment Currency\"], currency_dict)\n",
    "\n",
    "        payment_format = get_dict_value(raw_data[i, \"Payment Format\"], payment_format_dict)\n",
    "\n",
    "        from_acc_id_str = raw_data[i, \"From Bank\"] + raw_data[i, 2]\n",
    "        from_id = get_dict_value(from_acc_id_str, account_dict)\n",
    "\n",
    "        to_acc_id_str = raw_data[i, \"To Bank\"] + raw_data[i, 4]\n",
    "        to_id = get_dict_value(to_acc_id_str, account_dict)\n",
    "\n",
    "        amount_received = float(raw_data[i, \"Amount Received\"])\n",
    "        amount_paid = float(raw_data[i, \"Amount Paid\"])\n",
    "\n",
    "        is_laundering = int(raw_data[i, \"Is Laundering\"])\n",
    "        \n",
    "        data.append([i, from_id, to_id, timestamp, amount_paid, payment_currency, amount_received, receiving_currency,\n",
    "                     payment_format, is_laundering])\n",
    "        \n",
    "        # Creating a pandas DataFrame\n",
    "        pandas_df = pd.DataFrame(data, columns=['Index', 'From_ID', 'To_ID', 'Timestamp', 'Amount_Paid', 'Payment_Currency',\n",
    "                                     'Amount_Received', 'Receiving_Currency', 'Payment_Format', 'Is_Laundering'])\n",
    "\n",
    "        ddf = dd.from_pandas(pandas_df, npartitions=2)\n",
    "\n",
    "    return ddf, first_timestamp, currency_dict, payment_format_dict, bank_account_dict, account_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa5b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges_to_graph(G, ddf):\n",
    "    def add_edges(partition):\n",
    "        for index, row in partition.iterrows():\n",
    "            G.add_edge(row['From_ID'], row['To_ID'], \n",
    "                       timestamp=row['Timestamp'], \n",
    "                       amount_sent=row['Amount_Paid'], \n",
    "                       amount_received=row['Amount_Received'], \n",
    "                       received_currency=row['Receiving_Currency'], \n",
    "                       payment_format=row['Payment_Format'])\n",
    "\n",
    "    ddf.map_partitions(add_edges).compute()\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2413f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "def create_graph(ddf):\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    ddf = add_edges_to_graph(G, ddf)\n",
    "    \n",
    "    return G, ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec078748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(node):\n",
    "    features = {}\n",
    "    # Degree\n",
    "    features['degree'] = G.degree[node]\n",
    "    # In Degree\n",
    "    features['in_degree'] = G.in_degree[node]\n",
    "    # Out Degree\n",
    "    features['out_degree'] = G.out_degree[node]\n",
    "    # Clustering Coefficient\n",
    "    features['clustering_coefficient'] = nx.clustering(G, node)\n",
    "    # Degree Centrality\n",
    "    features['degree_centrality'] = nx.degree_centrality(G)[node]\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8405080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_trans_with_gf(transactions_ddf, graph_ddf):\n",
    "    \n",
    "    # Create a dictionary from graph_ddf for faster lookups\n",
    "    graph_dict = dict(zip(graph_ddf['Node'], graph_ddf[['degree', 'in_degree', 'out_degree', 'clustering_coefficient', 'degree_centrality']].values))\n",
    "    \n",
    "    def merge_partition(partition):\n",
    "        \n",
    "        for index, row in partition.iterrows():\n",
    "            \n",
    "            from_node = row['From_ID']\n",
    "            to_node = row['To_ID']\n",
    "            \n",
    "            if from_node in graph_dict:\n",
    "                graph_row = graph_dict[from_node]\n",
    "                partition.loc[index, 'from_degree'] = graph_row['degree']\n",
    "                partition.loc[index, 'from_in_degree'] = graph_row['in_degree']\n",
    "                partition.loc[index, 'from_out_degree'] = graph_row['out_degree']\n",
    "                partition.loc[index, 'from_clustering_coeff'] = graph_row['clustering_coefficient']\n",
    "                partition.loc[index, 'from_degree_centrality'] = graph_row['degree_centrality']\n",
    "                \n",
    "            if to_node in graph_dict:\n",
    "                graph_row = graph_dict[to_node]\n",
    "                partition.loc[index, 'to_degree'] = graph_row['degree']\n",
    "                partition.loc[index, 'to_in_degree'] = graph_row['in_degree']\n",
    "                partition.loc[index, 'to_out_degree'] = graph_row['out_degree']\n",
    "                partition.loc[index, 'to_clustering_coeff'] = graph_row['clustering_coefficient']\n",
    "                partition.loc[index, 'to_degree_centrality'] = graph_row['degree_centrality']\n",
    "                \n",
    "        return partition\n",
    "    \n",
    "    # Apply the function to each partition\n",
    "    merged_ddf = transactions_ddf.map_partitions(merge_partition)\n",
    "    \n",
    "    return merged_ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81cc9f9",
   "metadata": {},
   "source": [
    "# read data and train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d585020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"HI-Small_Trans.csv\"\n",
    "raw_data = dt.fread(input_file, columns=dt.str32, fill=True)\n",
    "\n",
    "# Convert the raw_data DataTable to a pandas DataFrame\n",
    "raw_data_df = raw_data.to_pandas()\n",
    "print(raw_data_df.head())\n",
    "# Splitting the raw_data into train and test sets\n",
    "train_df, test_df = train_test_split(raw_data_df, test_size=0.2, random_state=42, stratify=raw_data_df['Is Laundering'])\n",
    "\n",
    "# Convert the splits back to DataTable if necessary\n",
    "train_dt = dt.Frame(train_df)\n",
    "test_dt = dt.Frame(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438df5dd",
   "metadata": {},
   "source": [
    "# train set prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_preprocessed_ddf, first_timestamp, currency_dict, payment_format_dict, bank_account_dict, account_dict\n",
    "= initial_preprocessing(train_dt, first_timestamp = -1)\n",
    "initial_preprocessed_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e67b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "G, train_graph_ddf = create_graph(initial_preprocessed_ddf)\n",
    "train_graph_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d3c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of nodes:\", G.number_of_nodes())\n",
    "print(\"Number of edges:\", G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc96f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of unique nodes to a Dask DataFrame\n",
    "unique_nodes = list(set(train_graph_ddf['From_ID']).union(train_graph_ddf['To_ID']))\n",
    "\n",
    "#append to unique nodes whenever new accounts from test set come up\n",
    "unique_nodes_dd = dd.from_pandas(pd.DataFrame(unique_nodes, columns=['Node']), npartitions=2)\n",
    "\n",
    "# Apply extract_features function to each unique node\n",
    "graph_features = unique_nodes_dd.map_partitions(lambda df: df.apply(lambda row: extract_features(row['Node']), axis=1), meta={'degree': 'float64', 'in_degree': 'float64', 'out_degree': 'float64', 'clustering_coefficient': 'float64', 'degree_centrality': 'float64'})\n",
    "\n",
    "# Persist the result in memory\n",
    "graph_features = graph_features.persist()\n",
    "\n",
    "# Display the first few rows of the resulting Dask DataFrame\n",
    "print(graph_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e57383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns to transactions_ddf\n",
    "train_graph_ddf['from_degree'] = None\n",
    "train_graph_ddf['from_in_degree'] = None\n",
    "train_graph_ddf['from_out_degree'] = None\n",
    "train_graph_ddf['from_clustering_coeff'] = None\n",
    "train_graph_ddf['from_degree_centrality'] = None\n",
    "train_graph_ddf['to_degree'] = None\n",
    "train_graph_ddf['to_in_degree'] = None\n",
    "train_graph_ddf['to_out_degree'] = None\n",
    "train_graph_ddf['to_clustering_coeff'] = None\n",
    "train_graph_ddf['to_degree_centrality'] = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89349d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366f216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train_df = merge_trans_with_gf(train_graph_ddf, graph_features)\n",
    "# normalize the dataset then train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba51b15",
   "metadata": {},
   "source": [
    "# test set prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339af798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatable as dt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datatable import f, join, sort\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def initial_preprocessing(raw_data, first_timestamp, currency_dict, payment_format_dict, bank_account_dict, account_dict):\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    def get_dict_value(name, collection):\n",
    "        if name in collection:\n",
    "            value = collection[name]\n",
    "        else:\n",
    "            value = len(collection)\n",
    "            collection[name] = value\n",
    "        return value\n",
    "\n",
    "    for i in range(raw_data.nrows):\n",
    "        datetime_object = datetime.strptime(raw_data[i, \"Timestamp\"], '%Y/%m/%d %H:%M')\n",
    "        timestamp = datetime_object.timestamp()\n",
    "        day = datetime_object.day\n",
    "        month = datetime_object.month\n",
    "        year = datetime_object.year\n",
    "\n",
    "        if first_timestamp == -1:\n",
    "            start_time = datetime(year, month, day)\n",
    "            first_timestamp = start_time.timestamp() - 10\n",
    "\n",
    "        timestamp = timestamp - first_timestamp\n",
    "\n",
    "        receiving_currency = get_dict_value(raw_data[i, \"Receiving Currency\"], currency_dict)\n",
    "        payment_currency = get_dict_value(raw_data[i, \"Payment Currency\"], currency_dict)\n",
    "\n",
    "        payment_format = get_dict_value(raw_data[i, \"Payment Format\"], payment_format_dict)\n",
    "\n",
    "        from_acc_id_str = raw_data[i, \"From Bank\"] + raw_data[i, 2]\n",
    "        from_id = get_dict_value(from_acc_id_str, account_dict)\n",
    "\n",
    "        to_acc_id_str = raw_data[i, \"To Bank\"] + raw_data[i, 4]\n",
    "        to_id = get_dict_value(to_acc_id_str, account_dict)\n",
    "\n",
    "        amount_received = float(raw_data[i, \"Amount Received\"])\n",
    "        amount_paid = float(raw_data[i, \"Amount Paid\"])\n",
    "\n",
    "        is_laundering = int(raw_data[i, \"Is Laundering\"])\n",
    "        \n",
    "        data.append([i, from_id, to_id, timestamp, amount_paid, payment_currency, amount_received, receiving_currency,\n",
    "                     payment_format, is_laundering])\n",
    "        \n",
    "        # Creating a pandas DataFrame\n",
    "        pandas_df = pd.DataFrame(data, columns=['Index', 'From_ID', 'To_ID', 'Timestamp', 'Amount_Paid', 'Payment_Currency',\n",
    "                                     'Amount_Received', 'Receiving_Currency', 'Payment_Format', 'Is_Laundering'])\n",
    "\n",
    "        ddf = dd.from_pandas(pandas_df, npartitions=2)\n",
    "        \n",
    "    return ddf, first_timestamp, currency_dict, payment_format_dict, bank_account_dict, account_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909bff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_initial_preprocessed_ddf, first_timestamp, currency_dict, payment_format_dict, bank_account_dict, account_dict\n",
    "= initial_preprocessing(test_dt, first_timestamp)\n",
    "test_initial_preprocessed_ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graph_ddf = add_edge_to_graph(G, test_initial_preprocessed_ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes_test = list(set(test_graph_ddf['From_ID']).union(test_graph_ddf['To_ID']))\n",
    "\n",
    "#apunique_nodes_tesunique_nodes_testto unique nodes whenever new accounts from test set come up\n",
    "unique_nodes_dd = dd.from_pandas(pd.DataFrame(unique_nodes, columns=['Node']), npartitions=2)\n",
    "\n",
    "# Apply extract_features function to each unique node\n",
    "graph_features = unique_nodes_dd.map_partitions(lambda df: df.apply(lambda row: extract_features(row['Node']), axis=1), meta={'degree': 'float64', 'in_degree': 'float64', 'out_degree': 'float64', 'clustering_coefficient': 'float64', 'degree_centrality': 'float64'})\n",
    "\n",
    "# Persist the result in memory\n",
    "graph_features = graph_features.persist()\n",
    "\n",
    "# Display the first few rows of the resulting Dask DataFrame\n",
    "print(graph_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c5cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns to transactions_ddf\n",
    "test_graph_ddf['from_degree'] = None\n",
    "test_graph_ddf['from_in_degree'] = None\n",
    "test_graph_ddf['from_out_degree'] = None\n",
    "test_graph_ddf['from_clustering_coeff'] = None\n",
    "test_graph_ddf['from_degree_centrality'] = None\n",
    "test_graph_ddf['to_degree'] = None\n",
    "test_graph_ddf['to_in_degree'] = None\n",
    "test_graph_ddf['to_out_degree'] = None\n",
    "test_graph_ddf['to_clustering_coeff'] = None\n",
    "test_graph_ddf['to_degree_centrality'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b016c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_test_df = merge_trans_with_gf(test_graph_ddf, graph_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79336dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train + test without the graph features\n",
    "graph_features = extract_gf(all nodes from train)\n",
    "append to train\n",
    "model.fit(train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
