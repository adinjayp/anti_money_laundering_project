{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4iVzYtrznDW",
        "outputId": "d23d3a76-c62e-4b91-8e25-523f0ed604d2"
      },
      "outputs": [],
      "source": [
        "# pip install datatable\n",
        "# pip install networkx\n",
        "# pip install pandas\n",
        "# pip install tqdm\n",
        "# pip install google\n",
        "# pip install google-auth google-cloud-storage\n",
        "# pip install gcsfs\n",
        "# pip install fsspec\n",
        "# pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPLjiUNE22d6"
      },
      "source": [
        "# 1. This will be the data_preprocessing.py file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datatable as dt\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from datatable import f, join, sort\n",
        "import sys\n",
        "import os\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import google.auth\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2CaMENRdz5Xw",
        "outputId": "53ccce4f-c64f-4792-8aef-9388eff2b0de"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def preprocess_data(input_file):\n",
        "    \"\"\"\n",
        "    Preprocesses the input CSV file.\n",
        "\n",
        "    Args:\n",
        "    input_file (str): Path to the input CSV file.\n",
        "\n",
        "    Returns:\n",
        "    datatable.Frame: Preprocessed data frame.\n",
        "    \"\"\"\n",
        "    output_path = \"aml_preprocessed_train_data.csv\"\n",
        "\n",
        "    raw_data = dt.fread(input_file, columns=dt.str32, fill=True)\n",
        "    currency_dict = {}\n",
        "    payment_format_dict = {}\n",
        "    bank_account_dict = {}\n",
        "    account_dict = {}\n",
        "\n",
        "    def get_dict_value(name, collection):\n",
        "        if name in collection:\n",
        "            value = collection[name]\n",
        "        else:\n",
        "            value = len(collection)\n",
        "            collection[name] = value\n",
        "        return value\n",
        "\n",
        "    header = \"EdgeID,from_id,to_id,Timestamp,\\\n",
        "    Amount Sent,Sent Currency,Amount Received,Received Currency,\\\n",
        "    Payment Format,Is Laundering\\n\"\n",
        "\n",
        "    first_timestamp = -1\n",
        "\n",
        "    with open(output_path, 'w') as writer:\n",
        "        writer.write(header)\n",
        "        for i in range(raw_data.nrows):\n",
        "            datetime_object = datetime.strptime(raw_data[i, \"Timestamp\"], '%Y/%m/%d %H:%M')\n",
        "            timestamp = datetime_object.timestamp()\n",
        "            day = datetime_object.day\n",
        "            month = datetime_object.month\n",
        "            year = datetime_object.year\n",
        "\n",
        "            if first_timestamp == -1:\n",
        "                start_time = datetime(year, month, day)\n",
        "                first_timestamp = start_time.timestamp() - 10\n",
        "\n",
        "            timestamp = timestamp - first_timestamp\n",
        "\n",
        "            cur1 = get_dict_value(raw_data[i, \"Receiving Currency\"], currency_dict)\n",
        "            cur2 = get_dict_value(raw_data[i, \"Payment Currency\"], currency_dict)\n",
        "\n",
        "            fmt = get_dict_value(raw_data[i, \"Payment Format\"], payment_format_dict)\n",
        "\n",
        "            from_acc_id_str = raw_data[i, \"From Bank\"] + raw_data[i, 2]\n",
        "            from_id = get_dict_value(from_acc_id_str, account_dict)\n",
        "\n",
        "            to_acc_id_str = raw_data[i, \"To Bank\"] + raw_data[i, 4]\n",
        "            to_id = get_dict_value(to_acc_id_str, account_dict)\n",
        "\n",
        "            amount_received_orig = float(raw_data[i, \"Amount Received\"])\n",
        "            amount_paid_orig = float(raw_data[i, \"Amount Paid\"])\n",
        "\n",
        "            isl = int(raw_data[i, \"Is Laundering\"])\n",
        "\n",
        "            line = '%d,%d,%d,%d,%f,%d,%f,%d,%d,%d\\n' % \\\n",
        "                  (i, from_id, to_id, timestamp, amount_paid_orig, cur2, amount_received_orig, cur1, fmt, isl)\n",
        "\n",
        "            writer.write(line)\n",
        "\n",
        "    formatted_data = dt.fread(output_path)\n",
        "    formatted_data = formatted_data[:, :, sort(3)]\n",
        "\n",
        "    formatted_data.to_csv(output_path)\n",
        "    return formatted_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCcVU64R3DiM"
      },
      "source": [
        "# 2. This is the feature_extraction.py file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7SEZgbx0CZr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def extract_features(G, transactions_df):\n",
        "    \"\"\"\n",
        "    Extracts features from the transactions DataFrame.\n",
        "\n",
        "    Args:\n",
        "    G (networkx.DiGraph): Directed graph network.\n",
        "    transactions_df (pandas.DataFrame): DataFrame containing transaction data.\n",
        "\n",
        "    Returns:\n",
        "    pandas.DataFrame: DataFrame with extracted features.\n",
        "    \"\"\"\n",
        "    # Initialize an empty DataFrame to store features\n",
        "    transaction_features = pd.DataFrame()\n",
        "\n",
        "    # Iterate through each transaction and extract features\n",
        "    for _, row in transactions_df.iterrows():\n",
        "        features = extract_features_for_transaction(G, row)\n",
        "        transaction_features = transaction_features.append(features, ignore_index=True)\n",
        "\n",
        "    return transaction_features\n",
        "\n",
        "def extract_features_for_transaction(G, transaction):\n",
        "    \"\"\"\n",
        "    Extracts features for a single transaction.\n",
        "\n",
        "    Args:\n",
        "    G (networkx.DiGraph): Directed graph network.\n",
        "    transaction (pandas.Series): Series representing a single transaction.\n",
        "\n",
        "    Returns:\n",
        "    dict: Dictionary containing extracted features.\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    from_id = transaction['from_id']\n",
        "    to_id = transaction['to_id']\n",
        "    # Implement feature extraction logic for a single transaction\n",
        "    features['From Degree'] = G.degree[from_id]\n",
        "    features['To Degree'] = G.degree[to_id]\n",
        "    features['From clustering_coefficient'] = nx.clustering(G, from_id)\n",
        "    features['To clustering_coefficient'] = nx.clustering(G, from_id)\n",
        "    features['From degree_centrality'] = nx.degree_centrality(G)[from_id]\n",
        "    features['To degree_centrality'] = nx.degree_centrality(G)[to_id]\n",
        "    return features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOOj-bcd3ee7"
      },
      "source": [
        "# 3. This is the main.py file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "cPRdBl210HJs",
        "outputId": "cdcc4b38-52fe-4599-cbcc-8c2492e9019b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "# import data_preprocessing\n",
        "# import feature_extraction\n",
        "# import model_training\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def main(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Main function to orchestrate the workflow.\n",
        "\n",
        "    Args:\n",
        "    input_file (str): Path to the input CSV file.\n",
        "    output_file (str): Path to save the output CSV file.\n",
        "    \"\"\"\n",
        "    # Step 1: Data preprocessing\n",
        "    # raw_data = data_preprocessing.preprocess_data(input_file)\n",
        "\n",
        "\n",
        "    # def extract_data(bucket_name, filename):\n",
        "    #     csv_data = pd.read_csv('gs://' + bucket_name + '/' + filename, encoding='utf-8')\n",
        "    #     return csv_data\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    # input_file = extract_data(bucket_name, filename)\n",
        "    raw_data = preprocess_data(input_file)\n",
        "\n",
        "    # Step 2: Process data and create Directed Graph Network\n",
        "    G, transactions_df = process_data(raw_data)\n",
        "\n",
        "    # Step 3: Feature extraction\n",
        "    features_df = extract_features(G, transactions_df)\n",
        "\n",
        "    # Step 4: Model training\n",
        "    X = features_df  # Features\n",
        "    y = transactions_df['Is Laundering']  # Target variable\n",
        "    # \n",
        "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=True)\n",
        "    # model = model_training.train_model(features_df)\n",
        "\n",
        "    # Step 5: Save or further process the model and data\n",
        "    pass\n",
        "\n",
        "def process_data(raw_data):\n",
        "    \"\"\"\n",
        "    Processes the raw data to create a Directed Graph Network and DataFrame.\n",
        "\n",
        "    Args:\n",
        "    raw_data (datatable.Frame): Raw data frame.\n",
        "\n",
        "    Returns:\n",
        "    networkx.DiGraph: Directed graph network.\n",
        "    pandas.DataFrame: DataFrame containing transaction data.\n",
        "    \"\"\"\n",
        "    currency_dict = {}\n",
        "    payment_format_dict = {}\n",
        "    bank_account_dict = {}\n",
        "    account_dict = {}\n",
        "\n",
        "    transactions_df = pd.DataFrame(raw_data.to_dict())\n",
        "    G = nx.DiGraph()\n",
        "\n",
        "    for _, row in transactions_df.iterrows():\n",
        "        # Add nodes and edges to the graph\n",
        "        from_id = get_dict_value(row['From Bank'] + row['From_ID'], account_dict)\n",
        "        to_id = get_dict_value(row['To Bank'] + row['To_ID'], account_dict)\n",
        "        G.add_edge(from_id, to_id, timestamp=row['Timestamp'], amount_sent=row['Amount Sent'],\n",
        "                   amount_received=row['Amount Received'], received_currency=row['Receiving Currency'],\n",
        "                   payment_format=row['Payment Format'])\n",
        "\n",
        "    return G, transactions_df\n",
        "\n",
        "def get_dict_value(name, collection):\n",
        "    \"\"\"\n",
        "    Gets the value associated with a name in a collection or adds it if not present.\n",
        "\n",
        "    Args:\n",
        "    name (str): Name to retrieve or add.\n",
        "    collection (dict): Dictionary collection.\n",
        "\n",
        "    Returns:\n",
        "    int: Value associated with the name.\n",
        "    \"\"\"\n",
        "    if name in collection:\n",
        "        value = collection[name]\n",
        "    else:\n",
        "        value = len(collection)\n",
        "        collection[name] = value\n",
        "    return value\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    bucket_name ='aml_mlops_bucket'\n",
        "    input_file = \"HI-Small_Trans.csv\"\n",
        "    output_file = \"aml_preprocessed_train_data.csv\"\n",
        "    main(input_file, output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
